# My Development Notes

## Stream of Consciousness Notes (This will be all my own words with no AI)

Since Iris is all in on AI first development I am challenging myself to go with a more vibe code based process than I am used to. Usually I will make one update at a time while talking with the agent, allowing myself to fully understand each step. IN order to achieve as much as I can on this project, I am going to attempt to let the agent run a little bit more. I will still be checking in and consulting with it on its decisions, but I will try to not micro manage the AI. I am hoping this will allow me to have more of an impact in a short time. 

I have choose to use claude code assistant  in Jetbrains because I have some familiarity with it from Amazon (Although Amazon technically wraps our model with some internal layers). Since I have not worked on building an AI system before I relied on Claude to get some information to better understand exactly what the ask would be for this project to make sure I understood the goals and the stretch goals properly. 

Then I went into planning mode and asked Claude to give me the high level architecture. It asked me for my stack preferences and I decided that I would go with your teams recommendation of Django and Next.js because I wanted to match the environment I would be using on the job, and I could trust that it would be a good match for the project. Since I am letting Claude code for me my expertise on the language did not seem to be worth optimizing for.

It asked my for LLM for the backend and I selected Claude since I already had access. It then generated a high level architecture, complete with API routes and project structure. I reviewed the structure and it seemed very reasonable. Given the time constraints I didn't spend too long doing independent research for other options or technology stacks. My thinking with this project is that I want to learn as much as possible and have something to play with, I don't have expertize in RAG pipelines or some of these technology options and my assumption is that Claude will make a reasonable scaffolding that will allow me to build something and learn from it. Then after I have something working I can start investigating in optimizations to improve it. Since this is a small project and AI has changed the velocity at which we can code, it seems less important to me to be perfect the first time, since we can learn the lessons and quickly improve out implementation or if need be craft a new one.

I let claude finish the backend implementation and then I made sure to test a simple happy path. it seems like we are having an issue where the similarity score is coming across as low in test data that seems pretty explicit ti me. So I suspect that will be worth some energy debugging. But I am going to table it for now because lowering the threshold did work. I had a happy path testing, so now i think making a quick UI will make it easier to learn more.


I'm letting it install all the next js dependencies and run wild with an UI. I will test it and hopefully use it to better understand the whole build since there will be something to touch and the user experience will be more tangible.

The front end was created incredibly quickly and it looks very reasonable out of the box. I am going to work on getting Claude to write a lot of tests so that we can verify the behavior is the way that we suspect it should be before i go into any optimizations.

I am very impressed at how much claude can generate at a time and honestly it has been difficult to review it at the speed that it can generate. I still have time for stretch goals so I think I am going to continue to trust claude and then I will do an in depth review when it is finished. That way I can understand the whole context and I am not preventing Claude from generating

I had claude generate the last async stretch goal. it completed and wrote unit test that passed but i wanted to test it for real since we were mocking behavior. Claude has had some trouble with the environment getting celery to play nice with the other dependencies. It is working though it but this may be a weakness. I also felt like when it first ran into trouble testing it basically hand-waved and declared the incomplete tests a success. I could see this type of problem presenting challenges in other projects. It seems like claude fell back on an older python version which was causing some of the issues. To its credit it di
d say upgrading was an option but it suggested instead just trusting that the async behavior was functional. Also to be fair to it when it switched to the correct version th async behavior did work out of the box so it was correct. But I don't like that it wanted to skip the testing. 

I am a little over the 5 hour mark and I wna to honor the spirit of the exercise so I wont be implementing the front end changes that would be required to support the async changes that I started in the back end. I have an event this evening but I want to do a deep review of the full app as it is now that we have spun everything up. So my plan is to return tomorrow morning and reflect on the app with fresh eyes so I can see how valuable it is, and if there are any short comings

Okay I have returned the next day with fresh eyes to see where the holes are in the vibecoded project. the first one that I have found is that in the async processing it has steps that are NoOPs where it is just updating a progress bar for no reason. Upon more investigation its seems like it also has some redundant status handling and its not actually called. I know we didn't integrate it into the front end but it ran test of teh async pathway yesterday so im wondering what it was actually testing.

For the confidence score logic, we are just asking claude for its confidence. I am not sure if that is a reliable metric for confidence. I don't believe this is an advertized feature from Calude and i think we are just asking the LLM to guess. 

Okay another huge issue is that I was trying to test the end to end process manually and I am seeing errors uploading documents from the front end and creating RFPs. The views all look good cause we created test data via code, but the actual end to end processing doesn't yet work. This is something I should have tested properly before attempting to complete the stretch goals.

In the interest of time boxing correctly I am going to stop this experiment here. I spend ~5ish hours on it yesterday and ~90 mins reviewing and writing up my thoughts this morning. I was tempted to fix the errors but I think this exercise is more about the process than the output. Overall I have some takeaways about a vibecode first approach. I think while I was generating yesterday it felt super powerful. The planning mode was extremely helpful to get a high level picture of a process I had never done before. How quickly it generated a huge amount oif code also seemed impressive at the time. However, it was generating so fast that it felt hard to properly review it. The generation speed, and my instance that I approve every command that it ran for safety made it very hard to get into the deep thinking mind space that happens wen you are writing good code, and so I found myself opting to let Claude run with me babysitting it. This felt worse as a developer then when I have coded a file at a time alongside the agent. I felt like I do not have the deepest understanding of this code base, because so much was created and its hard to track all of the information in retrospect. I do not trust this code to go to production, and while I think we could get there I think it would require several more hours of deep diving to understand everything properly. Then we could work with Claude to fix the errors that we uncovered.

While there are definitely shortcomings I do think there is an opportunity for me to improve my process relative to my normal file by file approach. I think properly using these tools could lead to faster outcomes but I think I need more control over the process. I think RAG being a new process to me also played into this affect because its hard tro have options in a short time frame on something you haven't worked with before. So I think taking some time at the front to learn about the system in general without AI could have been helpful in shaping the AI's initial direction. I also think I needed to chunk the work with the AI better. Maybe I could still let it run atutomously, but I should have been doing more in depth approvals at each step before committing, instead of optimizing for more code generating. I think this would have given better opportunities to catch issues as they cropped up instead of letting the tech debt accumulate. I definitely should have run manual tests way earlier in the process. It might have also made sense to understand the project entirely. write integration tests then, spend time reviewing the integration tests to make sure they look correct, and then let the AI write code that would pass those tests. That way we could have more confidence that the code would be dependable.

Overall, I don't think this is a great RFP app yet, but I do think there are teh bones of something that could be improved. And I do think this was an interesting experience for myself in attempting an entirely vibe coded project, that I have learned from.

